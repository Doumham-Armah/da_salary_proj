{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException, StaleElementReferenceException\n",
    "from selenium import webdriver\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "exps = (NoSuchElementException, StaleElementReferenceException)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs(keyword, pages, verbose=False):\n",
    "    \n",
    "    \"\"\"Method to collect job listings from Glassdoor.\n",
    "        \n",
    "        Args: \n",
    "            keyword (string): the name of the job title for which you want to collect jobs\n",
    "            pages (int): the number of pages you want to collect(around 30 jobs/page)\n",
    "        \n",
    "        Returns: \n",
    "            pandas.dataframe: a dataframe with all the jobs and their details\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    \n",
    "    #give path to chromedriver\n",
    "    exec_path_mac = '/Users/mahmoudhamra/Dropbox/GitHub Projects/da_salary_proj/scraping-glassdoor-selenium-master/chromedriver'\n",
    "    exec_path_pc = 'C:/Users/mahmo/Dropbox/GitHub Projects/da_salary_proj/chromedriver'\n",
    "    driver = webdriver.Chrome(executable_path=exec_path_pc, options=options)\n",
    "    \n",
    "\n",
    "    driver.set_window_size(1120, 1000)\n",
    "    url = 'https://www.glassdoor.com/Job/' + keyword.split()[0] + '-' + keyword.split()[1] + '-jobs-SRCH_KO0,12.htm?includeNoSalaryJobs=true&sortBy=date_desc'    \n",
    "    driver.get(url)\n",
    "    df = []    \n",
    "    \n",
    "    #loop through pages\n",
    "    for i in range(pages):\n",
    "        \n",
    "        #if more than one page, load and get the next pages\n",
    "        if i>=1:\n",
    "            url = 'https://www.glassdoor.com/Job/data-analyst-jobs-SRCH_KO0,12_IP' + str(i+1) + '.htm?includeNoSalaryJobs=true&sortBy=date_desc'\n",
    "            driver.get(url)\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        driver.implicitly_wait(1)\n",
    "        \n",
    "        #collect job listings in the page (30 jobs/page)\n",
    "        jobs = driver.find_elements_by_xpath('.//*[@id=\"MainCol\"]/div[1]/ul/li')\n",
    "        \n",
    "        for job in jobs:\n",
    "            \n",
    "            print(\"---------------------------\")\n",
    "            \n",
    "            # click on job \n",
    "            job.click()\n",
    "            print(\"JOB BUTTON CLICKED\")\n",
    "            \n",
    "            #if sign up window pops up close it\n",
    "            try:\n",
    "                driver.find_element_by_css_selector('[alt=\"Close\"]').click()  #clicking to the X.\n",
    "                print(\"clicked the X button\")\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            \n",
    "            #get job_title\n",
    "            try:\n",
    "                job_title = job.find_element_by_xpath('//*[@id=\"JDCol\"]/div/article/div/div[1]/div/div/div[1]/div[3]/div[1]/div[2]').text\n",
    "            except exps:\n",
    "                job_title = np.nan\n",
    "                \n",
    "            #get rating\n",
    "            try:\n",
    "                rating = job.find_element_by_xpath('//*[@id=\"JDCol\"]/div/article/div/div[1]/div/div/div[1]/div[3]/div[1]/div[1]/span').text\n",
    "            except exps:\n",
    "                rating = np.nan\n",
    "            \n",
    "            #get salary_estimate  \n",
    "            try:\n",
    "                salary_estimate = job.find_element_by_xpath('//*[@id=\"JDCol\"]/div/article/div/div[1]/div/div/div[1]/div[3]/div[1]/div[4]/span').text\n",
    "            except exps:\n",
    "                salary_estimate = np.nan\n",
    "            \n",
    "            #get location\n",
    "            try:\n",
    "                location = job.find_element_by_xpath('//*[@id=\"JDCol\"]/div/article/div/div[1]/div/div/div[1]/div[3]/div[1]/div[3]').text\n",
    "            except exps:\n",
    "                location = np.nan\n",
    "                                \n",
    "\n",
    "            #Looping over Company Overview section and appending information to comp_info list\n",
    "            comp_info = []\n",
    "            elems = driver.find_elements_by_xpath('.//div[@id=\"EmpBasicInfo\"]//div//div[@class=\"d-flex flex-wrap\"]//div[@class=\"d-flex justify-content-start css-rmzuhb e1pvx6aw0\"]')\n",
    "            for element in elems:\n",
    "                try:\n",
    "                    driver.implicitly_wait(5)\n",
    "                    #taking key value pairs ex: 'size', 1001 to 5000 Employees etc.\n",
    "                    comp_info.append(element.find_element_by_xpath('.//span[@class=\"css-1taruhi e1pvx6aw1\"]').text)\n",
    "                    comp_info.append(element.find_element_by_xpath('.//span[@class=\"css-i9gxme e1pvx6aw2\"]').text)\n",
    "\n",
    "                except StaleElementReferenceException:\n",
    "                    print(\"##################StaleElementReferenceException##################\")\n",
    "                        \n",
    "                    \n",
    "            # Convert comp_info list to dictionary\n",
    "            it = iter(comp_info)\n",
    "            comp_info = dict(zip(it, it))\n",
    "            \n",
    "           \n",
    "            # take values from comp_info dict\n",
    "            try:\n",
    "                size = comp_info['Size']\n",
    "            except KeyError:\n",
    "                size = \"Unknown\"\n",
    "                \n",
    "                            \n",
    "            try:\n",
    "                founded = comp_info['Founded']\n",
    "            except KeyError:\n",
    "                founded = np.nan\n",
    "                \n",
    "\n",
    "            try:\n",
    "                type_of_ownership = comp_info['Type']\n",
    "            except KeyError:\n",
    "                type_of_ownership = np.nan\n",
    "                \n",
    "\n",
    "            try:\n",
    "                industry = comp_info['Industry']\n",
    "            except KeyError:\n",
    "                industry = np.nan\n",
    "                \n",
    "\n",
    "            try:\n",
    "                sector = comp_info['Sector']\n",
    "            except KeyError:\n",
    "                sector = np.nan\n",
    "                \n",
    "\n",
    "            try:\n",
    "                revenue = comp_info['Revenue']\n",
    "            except KeyError:\n",
    "                revenue = np.nan\n",
    "                \n",
    "            if verbose:\n",
    "                print('job_title:' , job_title)\n",
    "                print('salary_estimate:', salary_estimate)\n",
    "                print('location:', location)\n",
    "                print('rating:', rating)\n",
    "                print('size:', size)\n",
    "                print('founded:', founded)\n",
    "                print('type_of_ownership:', type_of_ownership)\n",
    "                print('industry:', industry)\n",
    "                print('sector:', sector)\n",
    "                print('revenue:', revenue)\n",
    "                \n",
    "                \n",
    "            print(\"---------------------------\")\n",
    "            \n",
    "            #append job details to df\n",
    "            df.append({\n",
    "            \"job_title\": job_title,\n",
    "            \"salary_estimate\": salary_estimate,\n",
    "            \"location\": location,\n",
    "            \"rating\" : rating,\n",
    "            \"size\" : size,\n",
    "            \"founded\" : founded,\n",
    "            \"type_of_ownership\" : type_of_ownership,\n",
    "            \"industry\" : industry,\n",
    "            \"sector\" : sector,\n",
    "            \"revenue\" : revenue})\n",
    "         \n",
    "    return pd.DataFrame(df)  #This line converts the dictionary object into a pandas DataFrame  \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = get_jobs(\"data analyst\", 3, verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('jobs3', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
